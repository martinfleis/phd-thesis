# GMM vs Kmeans
To illustrate the behaviour visually, take the following example (figure \ref{fig:artificial_data}) of a two-dimensional dataset with four known clusters. The clusters are of unequal size, density and shape. Because we do not know what properties will have DHC in hyperspace, it is safe to assume that they could be similarly complicated.

![Artificial two dimensional (x, y) dataset containing 4 known cluster.](source/figures/ch7/gmm_illustration_data.pdf "Artificial two dimensional dataset"){#fig:artificial_data width=100%}

Let us first check what would be the result of clustering using the k-means algorithm with k=4. The figure \ref{fig:artificial_kmeans} shows 4 clusters, but only one (0) being correctly labeled. The variable shape and density of the other three clusters together with the proximity unveil the weak points of the k-means algorithm. We can see that the purely distance-based definition does not provide the appropriate results.

![K-means clustering (k=4) of the artificial two dimensional (x, y) dataset containing four known clusters. Apart from the one cluster (0) which is separated, none was correctly distinguished.](source/figures/ch7/gmm_illustration_kmeans.pdf "K-means clustering of the artificial dataset"){#fig:artificial_kmeans width=100%}

#### Dimensionality issue
One way how to deal with a large number of characters is a reduction of dimensionality. Two of the most applied statistical methods to reduce the number of dimensions of data are Factor analysis (FA) REF and Principal component analysis (PCA) REF. Both are aiming to describe the dataset using the smaller number of *factors* or *principal components* (essentially dimensionless variables hard to interpret). The principal concept allowing both the generation of meaningful clusters and effective reduction of dimension is the correlation of original variables. That causes an issue in the reduction of the used morphometric dataset as it is designed to limit empirical correlations. Hence FA and PCA are expected to be not very efficient in reduction.

The preliminary tests of PCA on the complete dataset of contextual characters shows that to retain at least 95% of the variance, the model needs at least 147 principal components (Figure \ref{fig:pca_graph}). That is a significant reduction, but the ideal number of dimensions is approximately 30-50, so the reduction is not good enough. Using 30 principal components, the retained variance drops to 69%. For 50 components, the value would be 78%. Because there is no set acceptable rate of explained variance needed, without validation data, it is not possible to determine the acceptable number of components. The results might or might not offer a helpful reduction of dimension.

![Principal Component Analysis results on the contextual characters (n=296) on Prague data. The red line marks *optimal* 0.95 explained variance, green line denotes 147 principal components as a first value reaching 0.95.](source/figures/ch7/pca_graph.pdf "PCA results on the contextual characters on Prague data"){#fig:pca_graph}

Difference between 296 dimensions of the original dataset and 160 dimensions to keep at least 95% of variance might reduce computational demands, but at the same time complicates the interpretation of clusters where each of the 147 components is a black box without a morphological meaning. It is expected that GMM will be able to handle 296 dimensions, even though the computation might require more resources. The decision for this research is to skip dimensionality reduction unless GMM proves to struggle to identify clusters. In the further development of the method, it may be helpful to employ PCA. However, that is left for future exploration.